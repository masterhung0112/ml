{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN2LayerScratch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masterhung0112/ml/blob/master/NN2LayerScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KEciAOKItYD7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h8xqOdT7t9gX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Class **dlnet** that setups and initializes our network.\n",
        "- **X**: Holds the input layer, the data given to the network. X is a matrix that has as many rows as features and as many columns as samples we have available to train the network.\n",
        "- **Y**: Holds the desired output.\n",
        "- **Yh**: Holds the output that our network produces. It should have the same dimensions than **Y**, our desired target values. We initialize it to zero.\n",
        "- **L**: It holds the number of layers of our network, 2.\n",
        "- **dims**: Next, we define the number of neurons or units in each of our layers. We do this with a numpy array. The first component of the array is our input (which is not counted as a layer of the network). Our input will have 9 units because, as we will see in a bit, our data-set will have 9 useful features. Next, the first layer of the neural network will have 15 neurons, and our second and final layer will have 1 (the output of the network).\n",
        "- **param**: A Python dictionary that will hold the W and b parameters of each of the layers of the network.\n",
        "- **ch**: a cache variable, a python dictionary that will hold some intermediate calculations that we will need during the backward pass of the gradient descent algorithm.\n",
        "- **lr**: Our learning rate. This sets the speed at which the network will learn.\n",
        "- **sam**: The number of training samples we have.\n",
        "- **loss**: An array where we will store the loss value of the network every x iterations. The loss value expresses the difference between the predicted output of our network and the target one.\n",
        "\n",
        "The function **nInit**, which will initialize with random values the parameters of our network\n",
        "- **W1**: The number of rows is the number of hidden units of that layer, dims[1], and the number of columns is the number of features/rows of the previous layer (in this case X, our input data), dims[0].\n",
        "- **b1**: Same number of rows as W1 and a single column.\n",
        "- **W2**: The number of rows is the number of hidden units of that layer, dims[2], and the number of columns is again the number of rows of the input to that layer, dims[1].\n",
        "- **b2**: Same number of rows as W2 and a single column."
      ]
    },
    {
      "metadata": {
        "id": "4WbimOH3tsaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class dlnet:\n",
        "  def __init__(self, x, y):\n",
        "    self.X = x\n",
        "    self.Y = y\n",
        "    self.Yh = np.zeros((1, self.Y.shape[1]))\n",
        "    \n",
        "    self.L = 2\n",
        "    self.dims = [9, 15, 1]\n",
        "    \n",
        "    self.param = {}\n",
        "    self.ch = {}\n",
        "    self.grad = {}\n",
        "    \n",
        "    self.loss = {}\n",
        "    self.lr = 0.003\n",
        "    self.sam = self.Y.shape[1]\n",
        "  \n",
        "  # the function nInit, which will initialize with random values the parameters of our network\n",
        "  def nInit(self):    \n",
        "    np.random.seed(1)\n",
        "    self.param['W1'] = np.random.randn(self.dims[1], self.dims[0]) / np.sqrt(self.dims[0]) \n",
        "    self.param['b1'] = np.zeros((self.dims[1], 1))        \n",
        "    self.param['W2'] = np.random.randn(self.dims[2], self.dims[1]) / np.sqrt(self.dims[1]) \n",
        "    self.param['b2'] = np.zeros((self.dims[2], 1))                \n",
        "    return\n",
        "  \n",
        "  # non-linear activation function\n",
        "  def Sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "  \n",
        "  # non-linear activation function\n",
        "  def Relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "  \n",
        "  # the Loss, the distance between Yh and Y\n",
        "  # Cross-Entropy Loss Function\n",
        "  def nloss(self,Yh):\n",
        "      loss = (1./self.sam) * (-np.dot(self.Y, np.log(Yh).T) - np.dot(1-self.Y, np.log(1-Yh).T))    \n",
        "      return loss\n",
        "    \n",
        "  # Take the input of the network and pass it forwards through \n",
        "  # its different layers until it produces an output\n",
        "  #\n",
        "  # Multiply the weights of the first layer by the input data and \n",
        "  # add the first bias matrix , b1, to produce Z1. \n",
        "  # We then apply the Relu function to Z1 to produce A1.\n",
        "  #\n",
        "  # Multiply the weight matrix of the second layer by its input, \n",
        "  # A1 (the soutput of the first layer, which is the input of the second layer),\n",
        "  # and we add the second bias matrix, b2, in order to produce Z2. \n",
        "  # We then apply the Sigmoid function to Z2 to produce A2, \n",
        "  # which is in fact Yh, the output of the network.\n",
        "  #\n",
        "  # Ran our input data through the network and produced Yh, an output\n",
        "  #\n",
        "  # Z: represent the output of the computation of a layer\n",
        "  # A: represent the output of the activation function\n",
        "  def forward(self):    \n",
        "    # Layer 1\n",
        "    Z1 = self.param['W1'].dot(self.X) + self.param['b1'] \n",
        "    A1 = Relu(Z1)\n",
        "    self.ch['Z1'], self.ch['A1'] = Z1, A1\n",
        "    \n",
        "    # Layer 2\n",
        "    Z2 = self.param['W2'].dot(A1) + self.param['b2']  \n",
        "    A2 = Sigmoid(Z2)\n",
        "    self.ch['Z2'], self.ch['A2'] = Z2, A2\n",
        "    self.Yh = A2\n",
        "    loss = self.nloss(A2)\n",
        "    \n",
        "    return self.Yh, loss\n",
        "\n",
        "  def gd(self,X, Y, iter = 3000):\n",
        "    np.random.seed(1)                         \n",
        "\n",
        "    self.nInit()\n",
        "\n",
        "    for i in range(0, iter):\n",
        "      Yh, loss=self.forward()\n",
        "      self.backward()\n",
        "        \n",
        "      if i % 500 == 0:\n",
        "        print (\"Cost after iteration %i: %f\" %(i, loss))\n",
        "        self.loss.append(loss)\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d6NmdiI73RM4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Store the data in .csv format in your machine or online\n",
        "- Read the data using Pandas read_csv function\n",
        "- Then we proceed to clean and prepare the data, build our datasets and run gradient descent."
      ]
    },
    {
      "metadata": {
        "id": "wMb6HcUT3LrX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24edb658-39e9-4d60-a60c-ab57b2ee67e3"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  data_url = 'https://raw.githubusercontent.com/masterhung0112/ml/master/NN2LayerScratch-wisconsin-cancer-dataset.csv'\n",
        "  df = pd.read_csv(data_url, header=None)\n",
        "  df = df[~df[6].isin(['?'])]\n",
        "  df = df.astype(float)\n",
        "  df.iloc[:,10].replace(2, 0,inplace=True)\n",
        "  df.iloc[:,10].replace(4, 1,inplace=True)\n",
        "\n",
        "  df.head(3)\n",
        "  scaled_df=df\n",
        "  names = df.columns[0:10]\n",
        "  scaler = MinMaxScaler() \n",
        "  scaled_df = scaler.fit_transform(df.iloc[:,0:10]) \n",
        "  scaled_df = pd.DataFrame(scaled_df, columns=names)\n",
        "  \n",
        "  x=scaled_df.iloc[0:500,1:10].values.transpose()\n",
        "  y=df.iloc[0:500,10:].values.transpose()\n",
        "\n",
        "  xval=scaled_df.iloc[501:683,1:10].values.transpose()\n",
        "  yval=df.iloc[501:683,10:].values.transpose()\n",
        "\n",
        "  print(df.shape, x.shape, y.shape, xval.shape, yval.shape)\n",
        "\n",
        "  #nn = dlnet(x,y)\n",
        "  #nn.gd(x, y, iter = 15000)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(683, 11) (9, 500) (1, 500) (9, 182) (1, 182)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}