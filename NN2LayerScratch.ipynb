{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN2LayerScratch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masterhung0112/ml/blob/master/NN2LayerScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KEciAOKItYD7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h8xqOdT7t9gX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Class **dlnet** that setups and initializes our network.\n",
        "- **X**: Holds the input layer, the data given to the network. X is a matrix that has as many rows as features and as many columns as samples we have available to train the network.\n",
        "- **Y**: Holds the desired output.\n",
        "- **Yh**: Holds the output that our network produces. It should have the same dimensions than **Y**, our desired target values. We initialize it to zero.\n",
        "- **L**: It holds the number of layers of our network, 2.\n",
        "- **dims**: Next, we define the number of neurons or units in each of our layers. We do this with a numpy array. The first component of the array is our input (which is not counted as a layer of the network). Our input will have 9 units because, as we will see in a bit, our data-set will have 9 useful features. Next, the first layer of the neural network will have 15 neurons, and our second and final layer will have 1 (the output of the network).\n",
        "- **param**: A Python dictionary that will hold the W and b parameters of each of the layers of the network.\n",
        "- **ch**: a cache variable, a python dictionary that will hold some intermediate calculations that we will need during the backward pass of the gradient descent algorithm.\n",
        "- **lr**: Our learning rate. This sets the speed at which the network will learn.\n",
        "- **sam**: The number of training samples we have.\n",
        "- **loss**: An array where we will store the loss value of the network every x iterations. The loss value expresses the difference between the predicted output of our network and the target one.\n",
        "\n",
        "The function **nInit**, which will initialize with random values the parameters of our network\n",
        "- **W1**: The number of rows is the number of hidden units of that layer, dims[1], and the number of columns is the number of features/rows of the previous layer (in this case X, our input data), dims[0].\n",
        "- **b1**: Same number of rows as W1 and a single column.\n",
        "- **W2**: The number of rows is the number of hidden units of that layer, dims[2], and the number of columns is again the number of rows of the input to that layer, dims[1].\n",
        "- **b2**: Same number of rows as W2 and a single column."
      ]
    },
    {
      "metadata": {
        "id": "4WbimOH3tsaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class dlnet:\n",
        "  def __init__(self, x, y):\n",
        "    self.X = x\n",
        "    self.Y = y\n",
        "    self.Yh = np.zeros((1, self.Y.shape[1]))\n",
        "    \n",
        "    self.L = 2\n",
        "    self.dims = [9, 15, 1]\n",
        "    \n",
        "    self.param = {}\n",
        "    self.ch = {}\n",
        "    self.grad = {}\n",
        "    \n",
        "    self.loss = {}\n",
        "    self.lr = 0.003\n",
        "    self.sam = self.Y.shape[1]\n",
        "  \n",
        "  # the function nInit, which will initialize with random values the parameters of our network\n",
        "  def nInit(self):    \n",
        "    np.random.seed(1)\n",
        "    self.param['W1'] = np.random.randn(self.dims[1], self.dims[0]) / np.sqrt(self.dims[0]) \n",
        "    self.param['b1'] = np.zeros((self.dims[1], 1))        \n",
        "    self.param['W2'] = np.random.randn(self.dims[2], self.dims[1]) / np.sqrt(self.dims[1]) \n",
        "    self.param['b2'] = np.zeros((self.dims[2], 1))                \n",
        "    return\n",
        "  \n",
        "  # non-linear activation function\n",
        "  def Sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "  \n",
        "  # non-linear activation function\n",
        "  def Relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "  \n",
        "  # Take the input of the network and pass it forwards through \n",
        "  # its different layers until it produces an output\n",
        "  #\n",
        "  # Multiply the weights of the first layer by the input data and \n",
        "  # add the first bias matrix , b1, to produce Z1. \n",
        "  # We then apply the Relu function to Z1 to produce A1.\n",
        "  #\n",
        "  # Multiply the weight matrix of the second layer by its input, \n",
        "  # A1 (the soutput of the first layer, which is the input of the second layer),\n",
        "  # and we add the second bias matrix, b2, in order to produce Z2. \n",
        "  # We then apply the Sigmoid function to Z2 to produce A2, \n",
        "  # which is in fact Yh, the output of the network.\n",
        "  #\n",
        "  # Ran our input data through the network and produced Yh, an output\n",
        "  def forward(self):    \n",
        "    Z1 = self.param['W1'].dot(self.X) + self.param['b1'] \n",
        "    A1 = Relu(Z1)\n",
        "    self.ch['Z1'],self.ch['A1']=Z1,A1\n",
        "        \n",
        "    Z2 = self.param['W2'].dot(A1) + self.param['b2']  \n",
        "    A2 = Sigmoid(Z2)\n",
        "    self.ch['Z2'],self.ch['A2']=Z2,A2\n",
        "    self.Yh = A2\n",
        "    loss = self.nloss(A2)\n",
        "    return self.Yh, loss"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}