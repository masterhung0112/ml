{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN2LayerScratch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masterhung0112/ml/blob/master/NN2LayerScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KEciAOKItYD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "da6b38cb-2dcc-4c7c-f8f4-f71952d76747"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklehttps://cdn-images-1.medium.com/max/800/1*dx2AYvXVyPZ38TAiPeD9Aw.jpegarn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-41a18f62406a>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    from sklehttps://cdn-images-1.medium.com/max/800/1*dx2AYvXVyPZ38TAiPeD9Aw.jpegarn import preprocessing\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "rzntUZIKPPOr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/masterhung0112/ml/blob/master/NN2LayerScratch.jpeg?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "OGI0YGZ252jt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# non-linear activation function\n",
        "def Sigmoid(Z):\n",
        "  return 1/(1+np.exp(-Z))\n",
        "\n",
        "# non-linear activation function\n",
        "def Relu(Z):\n",
        "  return np.maximum(0,Z)\n",
        "\n",
        "# the derivatives of the Relu \n",
        "def dRelu(x):\n",
        "  x[x<=0] = 0\n",
        "  x[x>0] = 1\n",
        "  return x\n",
        "\n",
        "# the derivatives of the Sigmoid \n",
        "def dSigmoid(Z):\n",
        "  s = 1/(1+np.exp(-Z))\n",
        "  dZ = s * (1-s)\n",
        "  return dZ\n",
        "\n",
        "def plotCf(a,b,t):\n",
        "  cf =confusion_matrix(a,b)\n",
        "  plt.imshow(cf,cmap=plt.cm.Blues,interpolation='nearest')\n",
        "  plt.colorbar()\n",
        "  plt.title(t)\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Actual')\n",
        "  tick_marks = np.arange(len(set(a))) # length of classes\n",
        "  class_labels = ['0','1']\n",
        "  plt.xticks(tick_marks,class_labels)\n",
        "  plt.yticks(tick_marks,class_labels)\n",
        "  thresh = cf.max() / 2.\n",
        "  for i,j in itertools.product(range(cf.shape[0]),range(cf.shape[1])):\n",
        "      plt.text(j,i,format(cf[i,j],'d'),horizontalalignment='center',color='white' if cf[i,j] >thresh else 'black')\n",
        "  plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h8xqOdT7t9gX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Class **dlnet** that setups and initializes our network.\n",
        "- **X**: Holds the input layer, the data given to the network. X is a matrix that has as many rows as features and as many columns as samples we have available to train the network.\n",
        "- **Y**: Holds the desired output.\n",
        "- **Yh**: Holds the output that our network produces. It should have the same dimensions than **Y**, our desired target values. We initialize it to zero.\n",
        "- **L**: It holds the number of layers of our network, 2.\n",
        "- **dims**: Next, we define the number of neurons or units in each of our layers. We do this with a numpy array. The first component of the array is our input (which is not counted as a layer of the network). Our input will have 9 units because, as we will see in a bit, our data-set will have 9 useful features. Next, the first layer of the neural network will have 15 neurons, and our second and final layer will have 1 (the output of the network).\n",
        "- **param**: A Python dictionary that will hold the W and b parameters of each of the layers of the network.\n",
        "- **ch**: a cache variable, a python dictionary that will hold some intermediate calculations that we will need during the backward pass of the gradient descent algorithm.\n",
        "- **lr**: Our learning rate. This sets the speed at which the network will learn.\n",
        "- **sam**: The number of training samples we have.\n",
        "- **loss**: An array where we will store the loss value of the network every x iterations. The loss value expresses the difference between the predicted output of our network and the target one.\n",
        "\n",
        "The function **nInit**, which will initialize with random values the parameters of our network\n",
        "- **W1**: The number of rows is the number of hidden units of that layer, dims[1], and the number of columns is the number of features/rows of the previous layer (in this case X, our input data), dims[0].\n",
        "- **b1**: Same number of rows as W1 and a single column.\n",
        "- **W2**: The number of rows is the number of hidden units of that layer, dims[2], and the number of columns is again the number of rows of the input to that layer, dims[1].\n",
        "- **b2**: Same number of rows as W2 and a single column."
      ]
    },
    {
      "metadata": {
        "id": "4WbimOH3tsaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class dlnet:\n",
        "  def __init__(self, x, y):\n",
        "    self.X = x\n",
        "    self.Y = y\n",
        "    self.Yh = np.zeros((1, self.Y.shape[1]))\n",
        "    \n",
        "    self.L = 2\n",
        "    self.dims = [9, 15, 1]\n",
        "    \n",
        "    self.param = {}\n",
        "    self.ch = {}\n",
        "    self.grad = {}\n",
        "    \n",
        "    self.loss = []\n",
        "    self.lr = 0.003\n",
        "    self.sam = self.Y.shape[1]\n",
        "  \n",
        "  # the function nInit, which will initialize with random values the parameters of our network\n",
        "  def nInit(self):    \n",
        "    np.random.seed(1)\n",
        "    self.param['W1'] = np.random.randn(self.dims[1], self.dims[0]) / np.sqrt(self.dims[0]) \n",
        "    self.param['b1'] = np.zeros((self.dims[1], 1))        \n",
        "    self.param['W2'] = np.random.randn(self.dims[2], self.dims[1]) / np.sqrt(self.dims[1]) \n",
        "    self.param['b2'] = np.zeros((self.dims[2], 1))                \n",
        "    return\n",
        "  \n",
        "  # the Loss, the distance between Yh and Y\n",
        "  # Cross-Entropy Loss Function\n",
        "  def nloss(self,Yh):\n",
        "      loss = (1./self.sam) * (-np.dot(self.Y, np.log(Yh).T) - np.dot(1-self.Y, np.log(1-Yh).T))    \n",
        "      return loss\n",
        "  \n",
        "  # Take the input of the network and pass it forwards through \n",
        "  # its different layers until it produces an output\n",
        "  #\n",
        "  # Multiply the weights of the first layer by the input data and \n",
        "  # add the first bias matrix , b1, to produce Z1. \n",
        "  # We then apply the Relu function to Z1 to produce A1.\n",
        "  #\n",
        "  # Multiply the weight matrix of the second layer by its input, \n",
        "  # A1 (the soutput of the first layer, which is the input of the second layer),\n",
        "  # and we add the second bias matrix, b2, in order to produce Z2. \n",
        "  # We then apply the Sigmoid function to Z2 to produce A2, \n",
        "  # which is in fact Yh, the output of the network.\n",
        "  #\n",
        "  # Ran our input data through the network and produced Yh, an output\n",
        "  #\n",
        "  # Z: represent the output of the computation of a layer\n",
        "  # A: represent the output of the activation function\n",
        "  def forward(self):    \n",
        "    # Layer 1\n",
        "    Z1 = self.param['W1'].dot(self.X) + self.param['b1'] \n",
        "    A1 = Relu(Z1)\n",
        "    self.ch['Z1'], self.ch['A1'] = Z1, A1\n",
        "    \n",
        "    # Layer 2\n",
        "    Z2 = self.param['W2'].dot(A1) + self.param['b2']  \n",
        "    A2 = Sigmoid(Z2)\n",
        "    self.ch['Z2'], self.ch['A2'] = Z2, A2\n",
        "    self.Yh = A2\n",
        "    loss = self.nloss(A2)\n",
        "    \n",
        "    return self.Yh, loss\n",
        "  \n",
        "  # dLoss_Yh = — (Y/Yh — (1-Y)/(1-Yh)) | Loss = -(Y Log Yh + (1-Y) Log (1-Yh))\n",
        "  # dYh_Z2 = dSigmoid(Z2) | dSigmoid = sigmoid(x) * (1.0 — sigmoid(x)) | Yh = sigmoid (Z2)\n",
        "  # dLoss_Z2 = dLoss_Yh * dSigmoid(Z2)\n",
        "  # dLoss_A1 = W2 * dLoss_Z2 | dZ2_A1 = W2 | Z2 = W2 A1 + b2\n",
        "  # dA1_Z1= dRelu (Z1) | A1 = Relu (Z1)\n",
        "  # dLoss_Z1 = dLoss_A1 * dRelu (Z1)\n",
        "  # dLoss_W1= X * dLoss_Z1 | dZ1_W1 = X | Z1 = W1 X+ b1\n",
        "  #\n",
        "  # dLoss_Yh = — (np.divide(self.Y, self.Yh ) — np.divide(1 — self.Y, 1 — self.Yh)) \n",
        "  # dLoss_Z2 = dLoss_Yh * dSigmoid(self.ch[‘Z2’]) \n",
        "  # dLoss_A1 = np.dot(self.param[“W2”].T, dLoss_Z2) \n",
        "  # dLoss_Z1 = dLoss_A1 * dRelu(self.ch[‘Z1’]) \n",
        "  # dLoss_W1 = 1./self.X.shape[1] * np.dot(dLoss_Z1, self.X.T)\n",
        "  #\n",
        "  def backward(self):\n",
        "    dLoss_Yh = - (np.divide(self.Y, self.Yh) - np.divide(1 - self.Y, 1 - self.Yh))    \n",
        "        \n",
        "    dLoss_Z2 = dLoss_Yh * dSigmoid(self.ch['Z2'])    \n",
        "    dLoss_A1 = np.dot(self.param[\"W2\"].T,dLoss_Z2)\n",
        "    dLoss_W2 = 1./self.ch['A1'].shape[1] * np.dot(dLoss_Z2,self.ch['A1'].T)\n",
        "    dLoss_b2 = 1./self.ch['A1'].shape[1] * np.dot(dLoss_Z2, np.ones([dLoss_Z2.shape[1],1])) \n",
        "                        \n",
        "    dLoss_Z1 = dLoss_A1 * dRelu(self.ch['Z1'])        \n",
        "    dLoss_A0 = np.dot(self.param[\"W1\"].T,dLoss_Z1)\n",
        "    dLoss_W1 = 1./self.X.shape[1] * np.dot(dLoss_Z1,self.X.T)\n",
        "    dLoss_b1 = 1./self.X.shape[1] * np.dot(dLoss_Z1, np.ones([dLoss_Z1.shape[1],1]))  \n",
        "    \n",
        "    # The learning rate is a parameter that allows us to set how fast the network learns. \n",
        "    # We, therefore, modify our weights and biases by a quantity proportional to that learning rate.\n",
        "    self.param[\"W1\"] = self.param[\"W1\"] - self.lr * dLoss_W1\n",
        "    self.param[\"b1\"] = self.param[\"b1\"] - self.lr * dLoss_b1\n",
        "    self.param[\"W2\"] = self.param[\"W2\"] - self.lr * dLoss_W2\n",
        "    self.param[\"b2\"] = self.param[\"b2\"] - self.lr * dLoss_b2\n",
        "  \n",
        "  def gd(self, X, Y, iter = 3000):\n",
        "    np.random.seed(1)                         \n",
        "\n",
        "    self.nInit()\n",
        "\n",
        "    for i in range(0, iter):\n",
        "      Yh, loss = self.forward()\n",
        "      self.backward()\n",
        "        \n",
        "      if i % 500 == 0:\n",
        "        print (\"Cost after iteration %i: %f\" % (i, loss))\n",
        "        self.loss.append(loss)\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d6NmdiI73RM4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Store the data in .csv format in your machine or online\n",
        "- Read the data using Pandas read_csv function\n",
        "- Then we proceed to clean and prepare the data, build our datasets and run gradient descent."
      ]
    },
    {
      "metadata": {
        "id": "wMb6HcUT3LrX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "e2ce6ff5-ef49-4bcb-f898-0ada3b4248e4"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  data_url = 'https://raw.githubusercontent.com/masterhung0112/ml/master/NN2LayerScratch-wisconsin-cancer-dataset.csv'\n",
        "  df = pd.read_csv(data_url, header=None)\n",
        "  df = df[~df[6].isin(['?'])]\n",
        "  df = df.astype(float)\n",
        "  df.iloc[:,10].replace(2, 0,inplace=True)\n",
        "  df.iloc[:,10].replace(4, 1,inplace=True)\n",
        "\n",
        "  df.head(3)\n",
        "  scaled_df=df\n",
        "  names = df.columns[0:10]\n",
        "  scaler = MinMaxScaler() \n",
        "  scaled_df = scaler.fit_transform(df.iloc[:,0:10]) \n",
        "  scaled_df = pd.DataFrame(scaled_df, columns=names)\n",
        "  \n",
        "  x=scaled_df.iloc[0:500,1:10].values.transpose()\n",
        "  y=df.iloc[0:500,10:].values.transpose()\n",
        "\n",
        "  xval=scaled_df.iloc[501:683,1:10].values.transpose()\n",
        "  yval=df.iloc[501:683,10:].values.transpose()\n",
        "\n",
        "  print(df.shape, x.shape, y.shape, xval.shape, yval.shape)\n",
        "\n",
        "  nn = dlnet(x,y)\n",
        "  nn.gd(x, y, iter = 15000)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(683, 11) (9, 500) (1, 500) (9, 182) (1, 182)\n",
            "Cost after iteration 0: 0.673967\n",
            "Cost after iteration 500: 0.584499\n",
            "Cost after iteration 1000: 0.495482\n",
            "Cost after iteration 1500: 0.413320\n",
            "Cost after iteration 2000: 0.345230\n",
            "Cost after iteration 2500: 0.292046\n",
            "Cost after iteration 3000: 0.252091\n",
            "Cost after iteration 3500: 0.222430\n",
            "Cost after iteration 4000: 0.200422\n",
            "Cost after iteration 4500: 0.183966\n",
            "Cost after iteration 5000: 0.171452\n",
            "Cost after iteration 5500: 0.161803\n",
            "Cost after iteration 6000: 0.154208\n",
            "Cost after iteration 6500: 0.148172\n",
            "Cost after iteration 7000: 0.143280\n",
            "Cost after iteration 7500: 0.139259\n",
            "Cost after iteration 8000: 0.135921\n",
            "Cost after iteration 8500: 0.133109\n",
            "Cost after iteration 9000: 0.130704\n",
            "Cost after iteration 9500: 0.128626\n",
            "Cost after iteration 10000: 0.126809\n",
            "Cost after iteration 10500: 0.125205\n",
            "Cost after iteration 11000: 0.123780\n",
            "Cost after iteration 11500: 0.122504\n",
            "Cost after iteration 12000: 0.121338\n",
            "Cost after iteration 12500: 0.120254\n",
            "Cost after iteration 13000: 0.119266\n",
            "Cost after iteration 13500: 0.118358\n",
            "Cost after iteration 14000: 0.117523\n",
            "Cost after iteration 14500: 0.116749\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}